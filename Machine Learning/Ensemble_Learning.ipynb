{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ensemble Learning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_0EzH-L1yzrx"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import dataset"
      ],
      "metadata": {
        "id": "GiaJUOIj0NkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_moons\n",
        "\n",
        "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "#standard scaling\n",
        "X_train = (X_train - np.mean(X_train))/np.std(X_train)\n",
        "X_test = (X_test - np.mean(X_test))/np.std(X_test)"
      ],
      "metadata": {
        "id": "fJyPMH_e0BRr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is composed by the following feature"
      ],
      "metadata": {
        "id": "UTQ1ankJ0ap4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ensamble Learning\n",
        "combine few good predictors to get one more accurate one predctor (wisdom of the crowd)"
      ],
      "metadata": {
        "id": "LD6LPETe0lp1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Voting classifier\n",
        "aggregate the predictions of each classifier and predict the class that gets the most votes. This **majority-vote** classifier is called hard classifier"
      ],
      "metadata": {
        "id": "E7nBTpEv09tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC"
      ],
      "metadata": {
        "id": "GAcxSt8g0UhC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set three classifiers\n",
        "log_clf = LogisticRegression()\n",
        "rnd_clf = RandomForestClassifier()\n",
        "svm_clf = SVC()\n",
        "\n",
        "#aggregate the three classifiers \n",
        "voting_clf = VotingClassifier(\n",
        "    estimators = [(\"lr\",log_clf), (\"rf\",rnd_clf),(\"svc\",svm_clf)],\n",
        "    voting = \"hard\"\n",
        ")"
      ],
      "metadata": {
        "id": "sBTGPYBP1eTD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets look at each classifier accuracy on the test set"
      ],
      "metadata": {
        "id": "Jbf07UuM2-W8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "giT9Iyql2A31"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for clf in (log_clf ,rnd_clf ,svm_clf, voting_clf):\n",
        "  clf.fit(X_train,y_train)\n",
        "  y_preds = clf.predict(X_test)\n",
        "  print(f\"{clf.__class__.__name__} {accuracy_score(y_preds,y_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNNbOrWq322P",
        "outputId": "462cb0ed-e286-45a0-bbc6-43d0245ef221"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression 0.84\n",
            "RandomForestClassifier 0.92\n",
            "SVC 0.92\n",
            "VotingClassifier 0.928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The voting classifiers looks like the best amog the previous!\n",
        "If every classifier in the ensable has a method to ouptut the probabilities of each class, you can do the same with the entire ensamble using the \"soft\" method. Usually this obtains better results."
      ],
      "metadata": {
        "id": "-5akQS4Obcmj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bagging and Pasting"
      ],
      "metadata": {
        "id": "4b1jc3wGcu4D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These approaches use he same training algorithm for every predictor, but train them on different subsets of the training set. When sampling is performed **with** replacement, the method is called bagging, pasting otherwise"
      ],
      "metadata": {
        "id": "vqJxbq7lcy4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Bagging of decision trees\n",
        "bagging of decision trees, is able to generalize so much better than a single decision tree"
      ],
      "metadata": {
        "id": "-vd5RdWZdOdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "6j3KxqREcDnH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(), n_estimators = 500,\n",
        "    max_samples = 1.0, bootstrap = True, n_jobs = -1)\n",
        "\n",
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred = bag_clf.predict(X_test)\n",
        "accuracy_score(y_pred, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WC56BQrNdflx",
        "outputId": "75653612-2080-4288-859d-7b08e692a030"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.904"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Out of bag evaluation**\n",
        "Some instances may be sampled several times during bootstrapping, while others may not be sampled at all, these are called out-of-bag instances.\n",
        "\n",
        "Since a predictor never sees the oob instances during training, it can be evaluated on these instances, without the need for a separate validation set"
      ],
      "metadata": {
        "id": "3xsad4Y1ejft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(), n_estimators = 500,\n",
        "    max_samples = 1.0, bootstrap = True, n_jobs = -1, oob_score = True)\n",
        "\n",
        "bag_clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_wVkhaKeAeT",
        "outputId": "03e8d850-29a4-4f62-d8f9-1d975e57905a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=500,\n",
              "                  n_jobs=-1, oob_score=True)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bag_clf.oob_score_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f86Hu2CRfdEX",
        "outputId": "64d93291-4867-4a3a-8b32-540d9d09de4b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8986666666666666"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(bag_clf.predict(X_test), y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCqu5aNYfj34",
        "outputId": "6dff5ad4-34b9-459d-bf47-759ff585331c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.904"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The oob score its pretty accurate!"
      ],
      "metadata": {
        "id": "30BYBcJ6htff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Random Forest\n",
        "we can simply recrate the previous bagging using a RandomForestClassifier class\n"
      ],
      "metadata": {
        "id": "80JPHXhDhyts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rnd_clf = RandomForestClassifier(n_estimators= 500, n_jobs=-1)\n",
        "rnd_clf.fit(X_train, y_train)\n",
        "y_pred = rnd_clf.predict(X_test)\n",
        "accuracy_score(y_pred,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLSLoWcAhrWO",
        "outputId": "20ee52d0-7e20-40e6-da97-7b682d6f8bef"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.896"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is also possible to use random forests that uses random thresholds for each feature. These are called Extreme Random Forests and are provided by the scikit library as ExtraTrees.\n",
        "ExtraTrees trades more bias for lower variance.\n",
        "\n",
        "- The **bias** error is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).\n",
        "- The variance is an error from sensitivity to small fluctuations in the training set. High variance may result from an algorithm modeling the random noise in the training data (overfitting).\n"
      ],
      "metadata": {
        "id": "gJ_esPDX0NNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "rnd_clf = ExtraTreesClassifier(n_estimators= 500, n_jobs=-1)\n",
        "rnd_clf.fit(X_train, y_train)\n",
        "y_pred = rnd_clf.predict(X_test)\n",
        "accuracy_score(y_pred,y_test)"
      ],
      "metadata": {
        "id": "yDfgyMKPiQxB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ac83cfa-d24c-4c62-8012-7b5c2fd95b64"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.904"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forests can also be used for **Feature Selection**. In fact a feature is important if it is able to create subsets with as much purity as possible.\n",
        "We can access the feature importance via **feature_importances_** variable."
      ],
      "metadata": {
        "id": "KllublKa1OVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
        "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
        "\n",
        "for name,score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
        "  print(name,score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNGRci0A1ESQ",
        "outputId": "83325450-64b6-43ed-f7c0-64d4aad56076"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sepal length (cm) 0.10640985970854759\n",
            "sepal width (cm) 0.025253139106723152\n",
            "petal length (cm) 0.4333483985051396\n",
            "petal width (cm) 0.43498860267958966\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Boosting\n",
        "This is another ensemble solution. The most famous boost methods are AdaBoosting and Gradient Boost. "
      ],
      "metadata": {
        "id": "949Q6tOM2H5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##AdaBoost\n",
        "Each new predictor (model) in the esnemble should focus on correct the instances that its predecessor underfitted, weighting the missclassified instances.\n",
        "The boosting cannot be parallelized, because each predictor should wait for the previous one.\n",
        "In scikit learn the \"SAMME\" algorithm is used for multiclass labels AdaBoost. While \"SAMME.R\" relies on probabilities instead of predictions, usually performs better."
      ],
      "metadata": {
        "id": "0EIBjTPO2YP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ada_clf = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth = 1), n_estimators = 250,\n",
        "    algorithm = \"SAMME.R\", learning_rate = 0.5\n",
        ")\n",
        "\n",
        "ada_clf.fit(X_train,y_train)\n",
        "y_preds = ada_clf.predict(X_test)\n",
        "accuracy_score(y_preds, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_j3E7Ak1wKu",
        "outputId": "770043b9-361a-4949-8897-146cb2440e72"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.912"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gradient Boosting\n",
        "Similar to AdaBoosting but instead of working on the weights, each predictor tries to fit the residuals errors of the previous predictor.\n",
        "Let's see how implement it from scratch.\n",
        "The implementation can be optimized adopting an early stopping."
      ],
      "metadata": {
        "id": "42IVoZ-q4ZUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier"
      ],
      "metadata": {
        "id": "DTem1OMe39Za"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gbrt = GradientBoostingClassifier(max_depth = 2, n_estimators = 3, learning_rate = 1.0)\n",
        "\n",
        "gbrt.fit(X_train, y_train)\n",
        "y_preds = gbrt.predict(X_test)\n",
        "accuracy_score(y_preds, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5Ui-5YN4wBi",
        "outputId": "ef0acbe0-8285-4712-f5b2-6d4f23a857fe"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.904"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An optimized implementation of the gradient boosting is provided by the **XGBoost** library."
      ],
      "metadata": {
        "id": "CyOKqG3d7n2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost"
      ],
      "metadata": {
        "id": "RQCajDLx6Bw7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_reg = xgboost.XGBClassifier()\n",
        "\n",
        "xgb_reg.fit(X_train, y_train)\n",
        "y_preds = xgb_reg.predict(X_test)\n",
        "accuracy_score(y_preds,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dO25HucQ8Fj9",
        "outputId": "886b5d74-d49a-4318-e605-57032c801e8c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.92"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stacking\n",
        "This is the last ensemble method. Instead of aggregating the predictors with trivial methods like majority voting, we train a model to perform the aggregation. Each tree predicts a certain value, and the final predictor called blender or meta-learner takes these predictions and output the final value."
      ],
      "metadata": {
        "id": "wJFE-72m8c19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sMb6VWLv8TbP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}