{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dimensionality Reduction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Problem: when you have milions of features the train step can be very slow and computationally expensive, this is calle the **course of dimensionality**.\n",
        "Fortunatelly there are algorithms to reduce the number of features in a dataset.Reducing the nmber of features can also be useful for data visualization (in 2D and 3D). The main approches to dimensionality reduction are : projction and Manifold Learning.\n",
        "\n",
        "###**Projection**:\n",
        "Project all the train instances to make them lie in a sub-space of the original dimensions\n",
        "\n",
        "###**Manifold**:\n",
        "a d-dimensional manifold is a part of a n-dimensional space (d < n) that locally resembles a d-dimensional hyperplane. \n"
      ],
      "metadata": {
        "id": "kc7eqtv9IVV1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PCA\n",
        "\n",
        "Principal Component Analysis identifies the hyperplane that lies closest to the data, and then it projects thedata onto it.\n",
        "PCA identifies the axis that accounts for the largest amount of variance in the training set, so that contain much informatio.\n",
        "PCA is performed using the SVD matric decompositon, where V contain the components required"
      ],
      "metadata": {
        "id": "eUpZxZ4DLKB9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ncNFO8DqIGaU"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#original data, 3 features\n",
        "X = np.random.randn(1000,3)\n",
        "\n",
        "X_centered = X - X.mean(axis = 0)\n",
        "U, s , Vt = np.linalg.svd(X_centered)\n",
        "\n",
        "c1 = Vt.T[:,0]\n",
        "c2 = Vt.T[:,1]"
      ],
      "metadata": {
        "id": "kXjTo4WkLqby"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the methods provided by scikit learn we dont even need to center the data manually"
      ],
      "metadata": {
        "id": "qi2AWKUtMc8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components = 2)\n",
        "X2D = pca.fit_transform(X)"
      ],
      "metadata": {
        "id": "ZyYmMHSWLxt-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anothe important piece of information is given by the explained_variance_ratio_ variable, it tells us that the 38% of variance lies in the c1 components and 32% in c2"
      ],
      "metadata": {
        "id": "7bQdZk0CNXCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca.explained_variance_ratio_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQU0ddpNMxxP",
        "outputId": "82a11c2e-1238-4337-ffd4-249650d3a6ea"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.382874  , 0.32223075])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###How to choose the right number of dimensions\n",
        "Often is preferable to choose the number of dimensions that add up to a sufficiently large portion of the variance (e.g 95%). We can use a float to indicate this ratio."
      ],
      "metadata": {
        "id": "hlPdoBa-ORD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components= 0.95)\n",
        "X_reduced = pca.fit_transform(X)"
      ],
      "metadata": {
        "id": "y9WbcpqhNfud"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#in these case we need all the components to add up 95%, (Its not even enough)\n",
        "pca.explained_variance_ratio_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7Bl5uXcOwcR",
        "outputId": "3869bede-5ec8-4e3b-dd68-0cbd902c8f2b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.382874  , 0.32223075, 0.29489525])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Randomized PCA\n",
        "It is an algorithm that find an aproximation of the d components but it is computationally more efficient. Scikit learn use this automatically when the dataset is huge!\n"
      ],
      "metadata": {
        "id": "9kv1syIIPiJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnd_pca = PCA(n_components=2, svd_solver=\"randomized\")\n",
        "X_reduced = rnd_pca.fit_transform(X)"
      ],
      "metadata": {
        "id": "_f3vSxf-P6DM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Incremental PCA\n",
        "PCA requires the whole training set to fit in memory. With incremental pca (IPCA) you can feed the algorithm in mini-batches."
      ],
      "metadata": {
        "id": "iZdRX5QdRo8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import IncrementalPCA\n",
        "\n",
        "n_batches = 100\n",
        "inc_pca = IncrementalPCA(n_components = 2)\n",
        "for X_batch in np.array_split(X, n_batches):\n",
        "  inc_pca.partial_fit(X_batch)"
      ],
      "metadata": {
        "id": "F7Y0-fFZSQ_d"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Kernel PCA\n",
        "Uses the **kernel trick**, in order to use a more complex function that is able to separate the data in a higher dimensional space. Similar in what happens in SVM.\n"
      ],
      "metadata": {
        "id": "S4qCwn2kTRZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import KernelPCA\n",
        "\n",
        "rbf_pca = KernelPCA(n_components = 2, kernel = \"rbf\", gamma = 0.05)\n",
        "X_reduced = rbf_pca.fit_transform(X)"
      ],
      "metadata": {
        "id": "FvjU0FlRTf18"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}